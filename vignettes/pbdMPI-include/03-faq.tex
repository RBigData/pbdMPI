

\section[FAQs]{FAQs}
\label{sec:faqs}
\addcontentsline{toc}{section}{\thesection. FAQs}


\subsection[General]{General}
\label{sec:general}
\addcontentsline{toc}{subsection}{\thesubsection. General}

\begin{enumerate}
\item {\bf\color{blue} Q:}
      Do I need MPI knowledge to run \pkg{pbdMPI}? \\
      {\bf\color{blue} A:}
      Yes, but only the big picture, not the details. We provide
      several examples in \code{pbdMPI/inst/examples/test_spmd/} to
      introduce essential methods for learning MPI communication.

\item {\bf\color{blue} Q:}
      Can I run \pkg{pbdMPI} on my laptop locally? \\
      {\bf\color{blue} A:}
      Sure, as long as you have an MPI system. You even can run it on 1 CPU.

\item {\bf\color{blue} Q:}
      Does \pkg{pbdMPI} support Windows clusters? \\
      {\bf\color{blue} A:}
      Yes, the released binary currently supports MS-MPI.
      Currently, \pkg{pbdMPI} is built with
      'HPC Pack 2012 R2 MS-MPI Redistributable Package' which is available
      at \url{http://http://www.microsoft.com/en-us/download/}.
      For other MPI systems, users have to compile from source.

\item {\bf\color{blue} Q:}
      Can I run \pkg{pbdMPI} in OpenMPI and MPICH2 together? \\
      {\bf\color{blue} A:}
      No, you can have both OpenMPI and MPICH2 installed in your OS, but
      you are only allowed to run \pkg{pbdMPI} with one MPI system.
      Just pick one.

\item {\bf\color{blue} Q:}
      Does \pkg{pbdMPI} support any interactive mode? \\
      {\bf\color{blue} A:}
      No, but yes. Since \pkg{pbdMPI} version 0.3-0, there are two additional
      packages \pkg{pbdZMQ}~\citep{Chen2015pbdZMQpackage} and
      \pkg{pbdCS}~\citep{Schmidt2015pbdCSpackage}
      which provide servers-client interaction building upon
      \pkg{pbdMPI} for parallel computing.

      Originally, \pkg{pbdMPI} only considers batch execution and
      aims for programming with big data that do not fit on desktop
      platforms. We think that interaction with big data on a big
      machine is better handled with a client/server interface, where
      the server runs SPMD codes on big data and the client operates
      with reduced data representations.

      If you really need an interactive mode, such as for debugging,
      you can utilize \pkg{pbdMPI} scripts inside
      \pkg{Rmpi}. \pkg{Rmpi} mainly focuses on Manager/Workers
      computing environments, but can run SPMD codes on workers only
      with a few adjustments. See the ``Programming with Big Data in R''
      website for details at \url{http://r-pbd.org/}.

      Note that \pkg{pbdMPI} uses communicators different from
      \pkg{Rmpi}.  Be sure to free the memory correctly for both
      packages before quitting. \code{finalize(mpi.finalize = FALSE)}
      can free the memory allocated by \pkg{pbdMPI}, but does not
      terminate MPI before calling \code{mpi.quit} of \pkg{Rmpi}.

\item {\bf\color{blue} Q:}
      Can I write my own collective functions for my own data type? \\
      {\bf\color{blue} A:}
      Yes, S4 methods allow users to add their own data type, and functions.
      Quick examples can be found in \code{pbdMPI/inst/examples/test_s4/}.

\item {\bf\color{blue} Q:}
      Does \pkg{pbdMPI} support long vector or 64-bit integer? \\
      {\bf\color{blue} A:}
      See Section~\ref{sec:long_vector}.

\item {\bf\color{blue} Q:}
      Does \pkg{pbdMPI} support Amazon Web Services (AWS EC2)? \\
      {\bf\color{blue} A:}
      See \url{http://thirteen-01.stat.iastate.edu/snoweye/pbdr/?item=aws_ec2}
      for setting a cluster on AWS EC2.

\item {\bf\color{blue} Q:}
      Does \pkg{pbdMPI} support multiple nodes in VirtualBox? \\
      {\bf\color{blue} A:}
      See \url{http://thirteen-01.stat.iastate.edu/snoweye/pbdr/?item=multiple_nodes}
      for setting a cluster with two nodes in VirtualBox. It is extensible to
      multiple nodes by linked or full cloning with a few network modifications.
      A pure text file \code{multiple_nodes.txt} contains detail steps for the
      setting.

\item {\bf\color{blue} Q:}
      A simple \pkg{pbdMPI} testing code hangs but simple MPI pure C code is
      working? \\
      {\bf\color{blue} A:}
      If your VirtualBox has multiple adapters (for example, \code{eth0} for
      NAT/host, \code{eth1} using internal \code{192.168.*.*} for MPI
      communication), then you may consider to bring down \code{eth0} next.
\begin{Command}
sudo ip link set eth0 down
\end{Command}
      Further, you may also consider
      to consult network experts for IP and routing table configurations
      when multiple adapters are required.
      \proglang{R}/\proglang{Rscript} may not know multiple adapters nor
      how networking or routing table is setting up. It is just easier for
      MPI to use a single adapter, open all INPUT/OUTPUT/FORWARD ports,
      stop all firewall, etc. MPI is designed for high performance computing,
      so don't put too much extra stuffs to decline the performance.
      (Thanks for Alba Mart\'{i}nez-Ruiz and Cristina Monta\~{n}ola in
       Universidad Cat\'{o}lica de la Ssma. Concepci\'{o}n, chil
       providing errors and issues.)

\end{enumerate}


\subsection[Programming]{Programming}
\label{sec:programming}
\addcontentsline{toc}{subsection}{\thesubsection. Programming}


\begin{enumerate}
\item {\bf\color{blue} Q:}
      What are \pkg{pbdMPI}'s high level back-ends for embarrassingly parallel
      calculations? \\
      {\bf\color{blue} A:}
      See man pages and examples of \code{pbdLapply()}, \code{pbdSapply()},
      \code{pbdApply()}, and \code{task.pull()} for more details.
      Some options of those functions, such as \code{pbd.mode}, may be also
      useful for different data distribution in embarrassingly parallel
      calculations.

\item {\bf\color{blue} Q:}
      Can I run task jobs by using \pkg{pbdMPI}? \\
      {\bf\color{blue} A:}
      Yes, it is relatively straightforward for parallel
      tasks. Neither extra automatic functions nor further
      command/data communication is required.  In other words, SPMD is
      easier for Monte Carlo, bootstrap, MCMC simulation and
      statistical analysis for ultra-large datasets.
      A more efficient way, such as task pull parallelism, can be found
      in next Q\&A.

Example 1:
\begin{Code}[title=SPMD R Script]
  suppressMessages(library(pbdMPI, quietly = TRUE))
  init()

  id <- get.jid(total.tasks)

  ### Using a loop.
  for(i in id){
    ### Put independent task i script here.
  }

  ### Or using apply-like functions.
  lapply(id, function(i){
    ### Put independent task i script here.
  })

  finalize()
\end{Code}

Note that \code{id} gets different values on different processors,
accomplishing \code{total.tasks} across all processors. Also note that
any data and partial results are not shared across the processors
unless communicated.

Example 2:
\begin{Code}[title=SPMD R Script]
  suppressMessages(library(pbdMPI, quietly = TRUE))
  init()

  ### Directly using a loop.
  for(i in 1:total.tasks){
    if(i %% comm.size() == comm.rank()){
    ### Put independent task i script here.
  }
}

### Or using apply-like function.
lapply(1:total.tasks, function(i){
  if(i %% comm.size() == comm.rank()){
  ### Put independent task i script here.
}
})

finalize()
\end{Code}


\item {\bf\color{blue} Q:}
      Can I use unblocked send functions, such as
      \code{isend()}? Or, does \code{isend()} truly unblocked? \\
      {\bf\color{blue} A:}
      The answer is no for \pkg{pbdMPI} earlier than version 0.2-2, but
      it is changed since version 0.2-3. A temporary buffer list
      \code{SPMD.NB.BUFFER} is used to store all objects being sent by
      \code{isend()}. The buffer is created
      and cumulated in \code{.pbd_env}, but released as
      \code{wait()} is called. Although this may take some performance
      and space, this can avoid \code{gc()} and memory overwrite before
      actual sending is done.


\item {\bf\color{blue} Q:}
      Can I run un-barrier task jobs, such as task pull parallelism,
      by using \pkg{pbdMPI}? \\
      {\bf\color{blue} A:}
      Yes, it is relatively straightforward via \pkg{pbdMPI} API function
      \code{task.pull()} in SPMD. For example, the next is available
      in demo which has a user
      defined function \code{FUN()} run on workers, and master (rank 0)
      controls the task management.
\begin{Command}
mpiexec -np 4 Rscript -e "demo(task_pull,'pbdMPI',ask=F,echo=F)"
\end{Command}
\begin{Code}[title=SPMD R Script (task\_pull)]
### Initial.
suppressMessages(library(pbdMPI, quietly = TRUE))

### Examples.
FUN <- function(jid){
  Sys.sleep(1)
  jid * 10
}

ret <- task.pull(1:10, FUN)
comm.print(ret)

if(comm.rank() == 0){
  ret.jobs <- unlist(ret)
  ret.jobs <- ret.jobs[names(ret.jobs) == "ret"]
  print(ret.jobs)
}

### Finish.
finalize()
\end{Code}


\item {\bf\color{blue} Q:}
      What if I want to run task push or pull by using \pkg{pbdMPI}? \\
      {\bf\color{blue} A:}
      No problem. As in the two proceeding examples, task push or pull
      can be done in the same way by using rank 0 as the manager and
      the other ranks as workers. However, we do not recommend it
      except perhaps for inhomogeneous computing environments and
      independent jobs.

\item {\bf\color{blue} Q:}
      Are S4 methods more efficient? \\
      {\bf\color{blue} A:}
      Yes and No. S4 methods are a little less efficient than using
      \code{switch ... case ...} in \proglang{C}, but most default
      methods use \code{raw} with \code{un-} and \code{serialize}
      which may cost \code{3-10} times more than using
      \code{integer} or \code{double}.
      Instead of writing \proglang{C} code, it is easier to take
      advantage of S4 methods to extend to general R objects (\code{matrix},
      \code{array}, \code{list}, \code{data.frame}, and \code{class} ...)
      by communicating with basic data types
      (\code{integer} and \code{double}) and avoiding serialization.

\item {\bf\color{blue} Q:}
      Can I disable the MPI initialization of \pkg{pbdMPI} when I call
      \code{library(pbdMPI)}? \\
      {\bf\color{blue} A:}
      Yes, you can set a hidden variable \code{.__DISABLE_MPI_INIT__} in the
      \code{.GlobalEnv} before calling \code{library(pbdMPI)}.
      For example,
\begin{Code}[title=SPMD R Script]
assign(".__DISABLE_MPI_INIT__", TRUE, envir = .GlobalEnv)
library(pbdMPI)
ls(all.names = TRUE)
init()
ls(all.names = TRUE)
finalize(mpi.finalize = FALSE)
\end{Code}
      Note that we are *NOT* supposed to kill MPI in the \code{finalize} step
      if MPI is initialized by external applications. But some memory allocated
      by \pkg{pbdMPI} has to be free, \code{mpi.finalize = FALSE} is set above.

      To avoid some initialization issues of MPI, \pkg{pbdMPI} uses a
      different way than \pkg{Rmpi}. \pkg{pbdMPI} allows you to disable
      initializing communicators when loading the library, and later on you can
      call \code{init} to initialize or obtain communicators through
      \code{.__MPI_APTS__} as in the next question.

\item {\bf\color{blue} Q:}
      Can \pkg{pbdMPI} take or export communicators? \\
      {\bf\color{blue} A:}
      Yes, the physical memory address is set to the variable
      \code{.__MPI_APTS__} in the \code{.GlobalEnv} through a call to
      \code{init()}. The variable points to a structure containing MPI
      structure arrays preallocated while \pkg{pbdMPI} is loaded.
      \code{pbdMPI/src/pkg_*} provides a mechanism to take or export
      external/global variables at the \proglang{C} language level.

\end{enumerate}




\subsection[MPI Errors]{MPI Errors}
\label{sec:mpi_errors}
\addcontentsline{toc}{subsection}{\thesubsection. MPI Errors}


\begin{enumerate}
\item {\bf\color{blue} Q:} 
      If compilation successful, but load fails with segfault
\begin{Error}
** testing if installed package can be loaded
sh: line 1:  2905 Segmentation fault
'/usr/local/R/3.0.0/intel13/lib64/R/bin/R' --no-save --slave 2>&1 <
/tmp/RtmpGkncGK/file1e541c57190
ERROR: loading failed

*** caught segfault ***
address (nil), cause 'unknown'
\end{Error}
      {\bf\color{blue} A:}
      Basically, \pkg{pbdMPI} and all \proglang{pbdR} are tested and have
      stable configuration in GNU environment. However, other compilers are
      also possible such as Intel compiler. This message may come from the
      system of login node does not have a MPI system, MPI system is
      only allowed to be loaded in computing node, or MPI shared library is
      not loaded correctly and known to \proglang{R}. The solution is to use
      extra flag to \code{R CMD INSTALL --no-test-load pbdMPI*.tar.gz}, and
      use \code{export LD_PRELOAD=...} as the answer to the next question.


\item {\bf\color{blue} Q:}
      If installation fails with
%{\color{red}
\begin{Error}
Error in dyn.load(file, DLLpath = DLLpath, ...) :
  unable to load shared object '/.../pbdMPI/libs/pbdMPI.so':
  libmpi.so: cannot open shared object file: No such file or directory
\end{Error}
%}
      {\bf\color{blue} A:}
      OpenMPI may not be installed in the usual location, so the environment
      variable \code{LD_LIBRARY_PATH} should be set to the
      \code{libmpi.so} path, such as
\begin{Command}
export LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH
\end{Command}
where \code{/usr/local/openmpi/lib} should be replaced by the path to
\code{libmpi.so}.
Or, use \code{export LD_PRELOAD=...} to preload the MPI library if the
library name is not conventional, such as
\begin{Command}
export LD_PRELOAD=/usr/local/openmpi/lib/libmpi.so:$LD_PRELOAD
\end{Command}
Another solution may be to use the unix command \code{ldconfig} to setup the
correct path.

\item {\bf\color{blue} Q:}
      \pkg{pbdMPI} installs successfuly, but fails at initialization when
      calling the function \code{init()} with error message
%{\color{red}
\begin{Error}
/usr/lib/R/bin/exec/R: symbol lookup error:
/usr/lib/openmpi/lib/openmpi/mca_paffinity_linux.so: undefined symbol:
mca_base_param_reg_int
\end{Error}
%}
      {\bf\color{blue} A:}
      The linked library at installation may be different from the runtime
      library, especially when your system has more than one MPI systems.
      Since the library at installation is detected by
      \code{autoconf} (\code{configure}) and \code{automake} (\code{Makevars}),
      it can be linked with OpenMPI library, but MPICH2 or LAM/MPI is searched
      before OpenMPI according to \code{$PATH}.

      Solutions:
      \begin{itemize}
      \item Check which MPI system is your favorite to call. If you use
            OpenMPI, then you have to link with OpenMPI. Similarly, for
            MPICH2.
      \item Or, only kepp the MPI system you do like and drop others.
      \item Use \code{-}\code{--with-mpi-type} to specify the MPI type.
      \item Use \code{-}\code{--with-mpi-include} and
            \code{-}\code{--with-mpi-libpath} to
            specify the right version.
      \end{itemize}

\item {\bf\color{blue} Q:}
      (Mac) If installs successfully, but fails at initialization with
\begin{Error}
Library not loaded: /usr/lib/libmpi.0.dylib
\end{Error}
      {\bf\color{blue} A:}
      Please make sure the GNU compiler, \proglang{R}, OpenMPI, and
      \pkg{pbdMPI} are all built and installed under unified conditions,
      such as 64-bits environment. 32-bits \proglang{R} may not be able to
      load 64-bits OpenMPI nor \pkg{pbdMPI}.

\item {\bf\color{blue} Q:}
      (Linux) If OpenMPI \code{mpiexec} fails with
%{\color{red}
\begin{Error}
mca: base: component_find: unable to open
/.../openmpi/lib/openmpi/mca_paffinity_hwloc:
/.../openmpi/lib/openmpi/mca_paffinity_hwloc.so:
undefined symbol: opal_hwloc_topology (ignored)
...
mca: base: component_find: unable to open
/.../openmpi/lib/openmpi/mca_carto_auto_detect:
/.../openmpi/lib/openmpi/mca_carto_auto_detect.so:
undefined symbol: opal_carto_base_graph_get_host_graph_fn (ignored)
...
\end{Error}
%}
      {\bf\color{blue} A:}
      The linked MPI library \code{libmpi.so} may be missing or have a
      different name.
      OpenMPI builds shared/dynamic libraries by default and the target
      file \code{libmpi.so} is used by \code{pbdMPI/src/spmd.c}
      through \code{\#include <dlfcn.h>} and \code{dlopen(...)} in the file
      \code{pbdMPI/src/pkg_dl.c}.

      Solutions:
      \begin{itemize}
      \item Check if the path and version of \code{libmpi.so} are correct.
            In particular, one may have different MPI systems installed.
      \item When linking with \code{libmpi.so} in OpenMPI, one must run/load
            \pkg{pbdMPI} with OpenMPI's \code{libmpi.so}. The same for
            LAM/MPI and MPICH2.
      \item Use \code{export LD_PRELOAD=$PATH_TO_libmpi.so.*} in command mode.
      \item Use the file \code{/etc/ld.so.conf} and the command \code{ldconfig}
            to manage personal MPI installation.
      \item Or, recompile OpenMPI with a static library, and use
            \code{libmpi.a} instead.
      \end{itemize}


\item {\bf\color{blue} Q:}
      (Windows) If OpenMPI \code{mpiexec} fails with
%{\color{red}
\begin{Error}
ORTE_ERROR_LOG: Error in file ..\..\..\openmpi-1
.6\orte\mca\ess\hnp\ess_hnp_module.c at line 194
...
ORTE_ERROR_LOG: Error in file ..\..\..\openmpi-1
.6\orte\runtime\orte_init.c at line 128
...
\end{Error}
%}
      {\bf\color{blue} A:}
      Check if the network is unplugged, the network should be ``ON'' even on
      a single machine. At least, the status of network interface should be
      correct.


\item {\bf\color{blue} Q:}
      (Windows) If MPICH2 \code{mpiexec} fails with
%{\color{red}
\begin{Error}
c:\>"C:\Program Files\MPICH2\bin\mpiexec.exe" −np 2 Rscript C:\my_script.r
launch failed: CreateProcess(Rscript C:\my_script.r) on 
failed, error 2 - The system cannot find the file specified.
\end{Error}
%}
      {\bf\color{blue} A:}
      Please try to use \code{Rscript.exe} in windows system.


\item {\bf\color{blue} Q:}
      For MPICH2 users, if installation fails with
%{\color{red}
\begin{Error}
/usr/bin/ld: libmpich.a(comm_get_attr.o): relocation R_X86_64_32
against `MPIR_ThreadInfo' can not be used when making a shared
object; recompile with -fPIC
libmpich.a: could not read symbols: Bad value
collect2: ld returned 1 exit status
\end{Error}
%}
      {\bf\color{blue} A:}
      MPICH2 by default does not install a shared library which means
      \code{libmpich.so} is missing and \pkg{pbdMPI} trys to link with a
      static library \code{libmpich.a} instead. Try to recompile MPICH2 with
      a flag \code{-}\code{--enable-shared} and reinstall \pkg{pbdMPI} again.

\item {\bf\color{blue} Q:}
      For MPICH2 and MPICH3 users, if installation fails with
\begin{Error}
/usr/bin/ld: cannot find -lopa
collect2: error: ld returned 1 exit status
make: *** [pbdMPI.so] Error 1
ERROR: compilation failed for package 'pbdMPI'
\end{Error}
      {\bf\color{blue} A:}
      By default, \code{-lopa} is required for some systems. However, some
      systems may not have it and can be disable with a configuration flag
      when install \pkg{pbdMPI}, such as
      \code{R CMD INSTALL pbdMPI*.tar.gz --configure-args="--disable-opa"}.


\item {\bf\color{blue} Q:} 
      (MacOS 10.9.4 + OpenMPI 1.1.8) If compilation successful, but test load
      fails with MCA errors such as ``Symol not found''
\begin{Error}
** installing vignettes
   `pbdMPI-guide.Rnw'
** testing if installed package can be loaded
[??.??.??.??.??:??] mca: base: component_find: unable to open
/.../open-mpi/1.8.1/lib/openmpi/mca_allocator_basic:
dlopen(/.../open-mpi/1.8.1/lib/openmpi/mca_allocator_basic.so, 9):
Symbol not found: _ompi_free_list_item_t_class
  Referenced from: /.../open-mpi/1.8.1/lib/openmpi/mca_allocator_basic.so
  Expected in: flat namespace
 in /.../open-mpi/1.8.1/lib/openmpi/mca_allocator_basic.so (ignored)
\end{Error}
      {\bf\color{blue} A:}
      The potential problem here is that \code{mpicc --showme} provides
      extra information, such as multiple include and library paths, and
      \code{configure} is not able to parse correctly. Therefore, it is
      easier to manually specify correct paths via \code{--configure-args}
      to \proglang{R}.
      (Thanks for Eilidh Troup in University of Edinburgh, Scotland
       providing errors and solutions.)
\begin{Code}
$  mpicc --showme:compile
-I/usr/local/Cellar/open-mpi/1.8.1/include
$ mpicc --showme:link
-L/usr/local/opt/libevent/lib -L/usr/local/Cellar/open-mpi/1.8.1/lib -lmpi

$ R CMD INSTALL pbdMPI_0.2-4.tar.gz \
  --configure-args="--with-mpi-type=OPENMPI \
    --with-mpi-include=/usr/local/Cellar/open-mpi/1.8.1/include \
    --with-mpi-libpath=/usr/local/Cellar/open-mpi/1.8.1/lib"
\end{Code}
      Note that \code{ACX_MPI} is also a good solution to fix
      \code{configure.ac}, however, it may screw up other platforms,
      such as Solaris, and upset CRAN. Anyone is welcome to submit a
      thoughful solution.

\end{enumerate}


\subsection[Other Errors]{Other Errors}
\label{sec:other_errors}
\addcontentsline{toc}{subsection}{\thesubsection. Other Errors}


\begin{enumerate}
\item {\bf\color{blue} Q:} 
      \pkg{pbdMPI} is linked with \pkg{pbdPROF}~\citep{Chen2013pbdPROFpackage}
      and \pkg{mpiP}~\citep{mpiP}.
      (i.e. \code{-}\code{--enable-pbdPROF} is used in \pkg{pbdMPI} and
            \code{-}\code{--with-mpiP} is used in \pkg{pbdPROF}.)
      If \pkg{pbdMPI} compilation successful, but load fails with 
\begin{Error}
Error : .onLoad failed in loadNamespace() for 'pbdMPI', details:
  call: dyn.load(file, DLLpath = DLLpath, ...)
  error: unable to load shared object 'pbdMPI.so':
  pbdMPI/libs/pbdMPI.so: undefined symbol: _Ux86_64_getcontext
\end{Error}
      {\bf\color{blue} A:}
      Some prerequisite packages by \pkg{mpiP} is installed incorrectly.
      Reinstall \pkg{mpiP} by
\begin{Code}
./configure --disable-libunwind CPPFLAGS="-fPIC -I/usr/lib/openmpi/include" LDFLAGS="-L/usr/lib/openmpi/lib -lmpi"
\end{Code}
      and followed by reinstall \pkg{pbdPROF} and \pkg{pbdMPI}.

\end{enumerate}
